{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b14be2bd",
   "metadata": {},
   "source": [
    "# 1. 환경설정\n",
    "\n",
    "- 파이토치\n",
    "\n",
    "- 한국어를 영어로 번역하는 Seq2Seq(기본 모델, Attention 적용 모델)을 구현\n",
    "\n",
    "- JSON 파일(train_set.json, valid_set.json) 형식\n",
    "\n",
    "- 데이터셋 각 항목은 한국어 문장(\"ko\")과 영어 번역문(\"mt\")으로 구성\n",
    "\n",
    "- 적절한 토크나이저를 선택하여 한국어, 영어 문장을 토큰화\n",
    "\n",
    "- 필요한 경우 SOS, EOS, PAD, UNK 등의 특수 토큰을 정의\n",
    "\n",
    "- 한국어와 영어 각각의 어휘 사전 구성\n",
    "\n",
    "- Seq2Seq 모델: GRU 기반의 Encoder-Decoder 모델을 구현하고, Teacher Forcing 기법을 적용해 학습\n",
    "\n",
    "- Attention 모델: Attention(Bahdanau 혹은 Luong)을 적용한 디코더를 구현\n",
    "\n",
    "- 무작위 문장 쌍에 대해 모델의 번역 결과를 출력\n",
    "\n",
    "- 다양한 평가 지표(예: BLEU 점수) 도입"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba2d2613",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\" 디렉토리 지정 \"\"\"\n",
    "import sys\n",
    "if 'google.colab' in sys.modules:\n",
    "    from google.colab import drive; drive.mount('/content/drive')\n",
    "    import drive.MyDrive.develop.config_my_path as cc\n",
    "    cc.dir('projects/side/250625-250702_machine_translator')\n",
    "\n",
    "\"\"\" 라이브러리 호출 \"\"\"\n",
    "!pip install gensim\n",
    "\n",
    "import os, sys\n",
    "import zipfile\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "from gensim.models import Word2Vec, FastText\n",
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "import numpy as np\n",
    "import re\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import json\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import random\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader, TensorDataset, RandomSampler\n",
    "from konlpy.tag import Okt\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# JSON 파일 경로\n",
    "train_json_file_path = \"/content/drive/Shareddrives/스프린트(AI) 드라이브/트랙 Master 폴더/스프린트 미션 및 모범답안/data/translation/일상생활및구어체_한영_train_set.json\"\n",
    "valid_json_file_path = \"/content/drive/Shareddrives/스프린트(AI) 드라이브/트랙 Master 폴더/스프린트 미션 및 모범답안/data/translation/일상생활및구어체_한영_valid_set.json\"\n",
    "\n",
    "# JSON 파일 불러오기\n",
    "def load_json(file_path, max_samples=1000):\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "    return data[\"data\"][:max_samples]\n",
    "\n",
    "# 훈련 및 검증 데이터 로드\n",
    "data_train = load_json(train_json_file_path, max_samples=50000)\n",
    "data_valid = load_json(valid_json_file_path, max_samples=1000)\n",
    "\n",
    "# ko와 mt 데이터 추출\n",
    "ko_sentences_train = [item[\"ko\"] for item in data_train]\n",
    "mt_sentences_train = [item[\"mt\"] for item in data_train]\n",
    "ko_sentences_valid = [item[\"ko\"] for item in data_valid]\n",
    "mt_sentences_valid = [item[\"mt\"] for item in data_valid]\n",
    "\n",
    "# 한국어 및 영어 토크나이저\n",
    "tokenizer_ko = Okt().morphs\n",
    "tokenizer_en = word_tokenize\n",
    "\n",
    "## 문장 길이 분석\n",
    "ko_lengths = [len(tokenizer_ko(sent)) for sent in ko_sentences_train]\n",
    "en_lengths = [len(tokenizer_en(sent)) for sent in mt_sentences_train]\n",
    "all_lengths = ko_lengths + en_lengths\n",
    "\n",
    "# 한국어와 영어 중 가장 긴 문장의 길이 기준으로 MAX_LENGTH 설정\n",
    "MAX_LENGTH = max(max(ko_lengths), max(en_lengths)) + 1  # SOS, EOS 포함 고려\n",
    "print(f\"Max sequence length: {MAX_LENGTH}\")\n",
    "\n",
    "# 특수 토큰 정의\n",
    "SOS_token = 0\n",
    "EOS_token = 1\n",
    "PAD_token = 2\n",
    "UNK_token = 3\n",
    "\n",
    "class Lang:\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        # 초기에는 PAD, SOS, EOS, UNK 토큰을 미리 등록\n",
    "        self.word2index = {PAD_token: \"PAD\", SOS_token: \"SOS\", EOS_token: \"EOS\", UNK_token: \"<unk>\"}\n",
    "        self.index2word = {PAD_token: \"PAD\", SOS_token: \"SOS\", EOS_token: \"EOS\", UNK_token: \"<unk>\"}\n",
    "        self.word2count = {}\n",
    "        self.n_words = 4  # PAD, SOS, EOS, UNK 포함\n",
    "\n",
    "    def addSentence(self, sentence, tokenizer):\n",
    "        for word in tokenizer(sentence):\n",
    "            self.addWord(word)\n",
    "\n",
    "    def addWord(self, word):\n",
    "        if word not in self.word2index:\n",
    "            self.word2index[word] = self.n_words\n",
    "            self.index2word[self.n_words] = word\n",
    "            self.word2count[word] = 1\n",
    "            self.n_words += 1\n",
    "        else:\n",
    "            self.word2count[word] += 1\n",
    "\n",
    "# 데이터 준비\n",
    "def prepareData(lang1, lang2, tokenizer1, tokenizer2):\n",
    "    input_lang = Lang(lang1)\n",
    "    output_lang = Lang(lang2)\n",
    "    pairs = list(zip(ko_sentences_train, mt_sentences_train))\n",
    "    print(\"Read %s sentence pairs\" % len(pairs))\n",
    "    for pair in pairs:\n",
    "        input_lang.addSentence(pair[0], tokenizer1)\n",
    "        output_lang.addSentence(pair[1], tokenizer2)\n",
    "    return input_lang, output_lang, pairs\n",
    "\n",
    "input_lang, output_lang, pairs = prepareData(\"ko\", \"en\", tokenizer_ko, tokenizer_en)\n",
    "\n",
    "# 텐서 변환 및 데이터 로더 생성\n",
    "def tensorFromSentence(lang, sentence, tokenizer):\n",
    "    indexes = [SOS_token]\n",
    "    indexes += [lang.word2index.get(word, UNK_token) for word in tokenizer(sentence)[:MAX_LENGTH - 2]]\n",
    "    indexes.append(EOS_token)\n",
    "    # 길이 MAX_LENGTH에 맞춰 PAD 추가\n",
    "    while len(indexes) < MAX_LENGTH:\n",
    "        indexes.append(PAD_token)\n",
    "    return torch.tensor(indexes[:MAX_LENGTH], dtype=torch.long, device=device)\n",
    "\n",
    "def get_dataloader(batch_size):\n",
    "    input_tensors = [tensorFromSentence(input_lang, inp, tokenizer_ko) for inp, _ in pairs]\n",
    "    target_tensors = [tensorFromSentence(output_lang, tgt, tokenizer_en) for _, tgt in pairs]\n",
    "\n",
    "    input_tensors = torch.stack(input_tensors, dim=0)  # [num_samples, MAX_LENGTH]\n",
    "    target_tensors = torch.stack(target_tensors, dim=0)  # [num_samples, MAX_LENGTH]\n",
    "\n",
    "    dataset = TensorDataset(input_tensors, target_tensors)\n",
    "    train_sampler = RandomSampler(dataset)\n",
    "    train_dataloader = DataLoader(dataset, sampler=train_sampler, batch_size=batch_size)\n",
    "\n",
    "    print(f\"input_tensors.shape: {input_tensors.shape}, target_tensors.shape: {target_tensors.shape}\")\n",
    "    return train_dataloader\n",
    "\n",
    "train_dataloader = get_dataloader(batch_size=32)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a4fc910",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "\n",
    "# JSON 로딩\n",
    "with open('data/train_set.json', encoding='utf-8') as f:\n",
    "    raw_data = json.load(f)['data']\n",
    "\n",
    "# 원문 추출\n",
    "ko_sentences = [item['ko'].strip().replace('>', '') for item in raw_data]\n",
    "en_sentences = [item['mt'].strip().replace('>', '') for item in raw_data]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af76357b",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "ko_lens = [len(s) for s in ko_sentences]\n",
    "en_lens = [len(s) for s in en_sentences]\n",
    "\n",
    "print(f\"📏 한국어 길이 - 평균: {sum(ko_lens)/len(ko_lens):.2f}, 최대: {max(ko_lens)}, 최소: {min(ko_lens)}\")\n",
    "print(f\"📏 영어 길이 - 평균: {sum(en_lens)/len(en_lens):.2f}, 최대: {max(en_lens)}, 최소: {min(en_lens)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2228b20f",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "plt.hist(ko_lens, bins=30, alpha=0.6, label='Korean')\n",
    "plt.hist(en_lens, bins=30, alpha=0.6, label='English')\n",
    "plt.xlabel('문장 길이 (문자 수)')\n",
    "plt.ylabel('문장 수')\n",
    "plt.title('문장 길이 분포')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa17d9eb",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "plt.scatter(ko_lens, en_lens, alpha=0.4)\n",
    "plt.xlabel('한국어 문장 길이')\n",
    "plt.ylabel('영어 문장 길이')\n",
    "plt.title('문장 길이 상관관계')\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4776d4cb",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "empty_count = sum(1 for k, e in zip(ko_sentences, en_sentences) if not k or not e)\n",
    "print(f\"❗ 빈 문장 쌍 수: {empty_count}\")\n",
    "\n",
    "pair_set = set(zip(ko_sentences, en_sentences))\n",
    "print(f\"🧼 전체 샘플: {len(ko_sentences)}, 중복 제거 후: {len(pair_set)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8f75bad",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "domains = [item['domain'] for item in raw_data]\n",
    "domain_count = Counter(domains)\n",
    "\n",
    "plt.bar(domain_count.keys(), domain_count.values())\n",
    "plt.xticks(rotation=45)\n",
    "plt.title(\"도메인 분포\")\n",
    "plt.ylabel(\"샘플 수\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7372bfb",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "styles = [item['style'] for item in raw_data if item['style']]\n",
    "style_count = Counter(styles)\n",
    "\n",
    "print(\"🎭 문체 분포:\", style_count)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aa30e67",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "140b576b",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c26e9d16",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deef7085",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from kiwipiepy import Kiwi\n",
    "import spacy\n",
    "\n",
    "def load_and_tokenize(path):\n",
    "    kiwi = Kiwi()\n",
    "    en_tokenizer = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "    def tokenize_ko(text):\n",
    "        return [token.form for token in kiwi.tokenize(text)]\n",
    "\n",
    "    def tokenize_en(text):\n",
    "        return [token.text.lower() for token in en_tokenizer(text)]\n",
    "\n",
    "    with open(path, encoding='utf-8') as f:\n",
    "        raw = json.load(f)['data']\n",
    "    \n",
    "    ko_raw, en_raw = [], []\n",
    "    ko_tokenized, en_tokenized = [], []\n",
    "\n",
    "    for item in raw:\n",
    "        ko = item['ko'].strip().replace('>', '')\n",
    "        en = item['mt'].strip().replace('>', '')\n",
    "        ko_raw.append(ko)\n",
    "        en_raw.append(en)\n",
    "        ko_tokenized.append(tokenize_ko(ko))\n",
    "        en_tokenized.append(tokenize_en(en))\n",
    "    \n",
    "    return ko_raw, en_raw, ko_tokenized, en_tokenized, raw\n",
    "\n",
    "\n",
    "ko_raw, en_raw, ko_tok, en_tok, raw_json = load_and_tokenize(\"data/train_set.json\")\n",
    "\n",
    "print(\"✅ 한국어 원문:\", ko_raw[0])\n",
    "print(\"✅ 한국어 토큰:\", ko_tok[0])\n",
    "print(\"✅ 영어 원문:\", en_raw[0])\n",
    "print(\"✅ 영어 토큰:\", en_tok[0])\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
