{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b14be2bd",
   "metadata": {},
   "source": [
    "# 1. í™˜ê²½ì„¤ì •\n",
    "\n",
    "- íŒŒì´í† ì¹˜\n",
    "\n",
    "- í•œêµ­ì–´ë¥¼ ì˜ì–´ë¡œ ë²ˆì—­í•˜ëŠ” Seq2Seq(ê¸°ë³¸ ëª¨ë¸, Attention ì ìš© ëª¨ë¸)ì„ êµ¬í˜„\n",
    "\n",
    "- JSON íŒŒì¼(train_set.json, valid_set.json) í˜•ì‹\n",
    "\n",
    "- ë°ì´í„°ì…‹ ê° í•­ëª©ì€ í•œêµ­ì–´ ë¬¸ì¥(\"ko\")ê³¼ ì˜ì–´ ë²ˆì—­ë¬¸(\"mt\")ìœ¼ë¡œ êµ¬ì„±\n",
    "\n",
    "- ì ì ˆí•œ í† í¬ë‚˜ì´ì €ë¥¼ ì„ íƒí•˜ì—¬ í•œêµ­ì–´, ì˜ì–´ ë¬¸ì¥ì„ í† í°í™”\n",
    "\n",
    "- í•„ìš”í•œ ê²½ìš° SOS, EOS, PAD, UNK ë“±ì˜ íŠ¹ìˆ˜ í† í°ì„ ì •ì˜\n",
    "\n",
    "- í•œêµ­ì–´ì™€ ì˜ì–´ ê°ê°ì˜ ì–´íœ˜ ì‚¬ì „ êµ¬ì„±\n",
    "\n",
    "- Seq2Seq ëª¨ë¸: GRU ê¸°ë°˜ì˜ Encoder-Decoder ëª¨ë¸ì„ êµ¬í˜„í•˜ê³ , Teacher Forcing ê¸°ë²•ì„ ì ìš©í•´ í•™ìŠµ\n",
    "\n",
    "- Attention ëª¨ë¸: Attention(Bahdanau í˜¹ì€ Luong)ì„ ì ìš©í•œ ë””ì½”ë”ë¥¼ êµ¬í˜„\n",
    "\n",
    "- ë¬´ì‘ìœ„ ë¬¸ì¥ ìŒì— ëŒ€í•´ ëª¨ë¸ì˜ ë²ˆì—­ ê²°ê³¼ë¥¼ ì¶œë ¥\n",
    "\n",
    "- ë‹¤ì–‘í•œ í‰ê°€ ì§€í‘œ(ì˜ˆ: BLEU ì ìˆ˜) ë„ì…"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba2d2613",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\" ë””ë ‰í† ë¦¬ ì§€ì • \"\"\"\n",
    "import sys\n",
    "if 'google.colab' in sys.modules:\n",
    "    from google.colab import drive; drive.mount('/content/drive')\n",
    "    import drive.MyDrive.develop.config_my_path as cc\n",
    "    cc.dir('projects/side/250625-250702_machine_translator')\n",
    "\n",
    "\"\"\" ë¼ì´ë¸ŒëŸ¬ë¦¬ í˜¸ì¶œ \"\"\"\n",
    "!pip install gensim\n",
    "\n",
    "import os, sys\n",
    "import zipfile\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "from gensim.models import Word2Vec, FastText\n",
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "import numpy as np\n",
    "import re\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import json\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import random\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader, TensorDataset, RandomSampler\n",
    "from konlpy.tag import Okt\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# JSON íŒŒì¼ ê²½ë¡œ\n",
    "train_json_file_path = \"/content/drive/Shareddrives/á„‰á…³á„‘á…³á„…á…µá†«á„á…³(AI) á„ƒá…³á„…á…¡á„‹á…µá„‡á…³/á„á…³á„…á…¢á†¨ Master á„‘á…©á†¯á„ƒá…¥/ìŠ¤í”„ë¦°íŠ¸ ë¯¸ì…˜ ë° ëª¨ë²”ë‹µì•ˆ/data/translation/á„‹á…µá†¯á„‰á…¡á†¼á„‰á…¢á†¼á„’á…ªá†¯á„†á…µá†¾á„€á…®á„‹á…¥á„á…¦_á„’á…¡á†«á„‹á…§á†¼_train_set.json\"\n",
    "valid_json_file_path = \"/content/drive/Shareddrives/á„‰á…³á„‘á…³á„…á…µá†«á„á…³(AI) á„ƒá…³á„…á…¡á„‹á…µá„‡á…³/á„á…³á„…á…¢á†¨ Master á„‘á…©á†¯á„ƒá…¥/ìŠ¤í”„ë¦°íŠ¸ ë¯¸ì…˜ ë° ëª¨ë²”ë‹µì•ˆ/data/translation/á„‹á…µá†¯á„‰á…¡á†¼á„‰á…¢á†¼á„’á…ªá†¯á„†á…µá†¾á„€á…®á„‹á…¥á„á…¦_í•œì˜_valid_set.json\"\n",
    "\n",
    "# JSON íŒŒì¼ ë¶ˆëŸ¬ì˜¤ê¸°\n",
    "def load_json(file_path, max_samples=1000):\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "    return data[\"data\"][:max_samples]\n",
    "\n",
    "# í›ˆë ¨ ë° ê²€ì¦ ë°ì´í„° ë¡œë“œ\n",
    "data_train = load_json(train_json_file_path, max_samples=50000)\n",
    "data_valid = load_json(valid_json_file_path, max_samples=1000)\n",
    "\n",
    "# koì™€ mt ë°ì´í„° ì¶”ì¶œ\n",
    "ko_sentences_train = [item[\"ko\"] for item in data_train]\n",
    "mt_sentences_train = [item[\"mt\"] for item in data_train]\n",
    "ko_sentences_valid = [item[\"ko\"] for item in data_valid]\n",
    "mt_sentences_valid = [item[\"mt\"] for item in data_valid]\n",
    "\n",
    "# í•œêµ­ì–´ ë° ì˜ì–´ í† í¬ë‚˜ì´ì €\n",
    "tokenizer_ko = Okt().morphs\n",
    "tokenizer_en = word_tokenize\n",
    "\n",
    "## ë¬¸ì¥ ê¸¸ì´ ë¶„ì„\n",
    "ko_lengths = [len(tokenizer_ko(sent)) for sent in ko_sentences_train]\n",
    "en_lengths = [len(tokenizer_en(sent)) for sent in mt_sentences_train]\n",
    "all_lengths = ko_lengths + en_lengths\n",
    "\n",
    "# í•œêµ­ì–´ì™€ ì˜ì–´ ì¤‘ ê°€ì¥ ê¸´ ë¬¸ì¥ì˜ ê¸¸ì´ ê¸°ì¤€ìœ¼ë¡œ MAX_LENGTH ì„¤ì •\n",
    "MAX_LENGTH = max(max(ko_lengths), max(en_lengths)) + 1  # SOS, EOS í¬í•¨ ê³ ë ¤\n",
    "print(f\"Max sequence length: {MAX_LENGTH}\")\n",
    "\n",
    "# íŠ¹ìˆ˜ í† í° ì •ì˜\n",
    "SOS_token = 0\n",
    "EOS_token = 1\n",
    "PAD_token = 2\n",
    "UNK_token = 3\n",
    "\n",
    "class Lang:\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        # ì´ˆê¸°ì—ëŠ” PAD, SOS, EOS, UNK í† í°ì„ ë¯¸ë¦¬ ë“±ë¡\n",
    "        self.word2index = {PAD_token: \"PAD\", SOS_token: \"SOS\", EOS_token: \"EOS\", UNK_token: \"<unk>\"}\n",
    "        self.index2word = {PAD_token: \"PAD\", SOS_token: \"SOS\", EOS_token: \"EOS\", UNK_token: \"<unk>\"}\n",
    "        self.word2count = {}\n",
    "        self.n_words = 4  # PAD, SOS, EOS, UNK í¬í•¨\n",
    "\n",
    "    def addSentence(self, sentence, tokenizer):\n",
    "        for word in tokenizer(sentence):\n",
    "            self.addWord(word)\n",
    "\n",
    "    def addWord(self, word):\n",
    "        if word not in self.word2index:\n",
    "            self.word2index[word] = self.n_words\n",
    "            self.index2word[self.n_words] = word\n",
    "            self.word2count[word] = 1\n",
    "            self.n_words += 1\n",
    "        else:\n",
    "            self.word2count[word] += 1\n",
    "\n",
    "# ë°ì´í„° ì¤€ë¹„\n",
    "def prepareData(lang1, lang2, tokenizer1, tokenizer2):\n",
    "    input_lang = Lang(lang1)\n",
    "    output_lang = Lang(lang2)\n",
    "    pairs = list(zip(ko_sentences_train, mt_sentences_train))\n",
    "    print(\"Read %s sentence pairs\" % len(pairs))\n",
    "    for pair in pairs:\n",
    "        input_lang.addSentence(pair[0], tokenizer1)\n",
    "        output_lang.addSentence(pair[1], tokenizer2)\n",
    "    return input_lang, output_lang, pairs\n",
    "\n",
    "input_lang, output_lang, pairs = prepareData(\"ko\", \"en\", tokenizer_ko, tokenizer_en)\n",
    "\n",
    "# í…ì„œ ë³€í™˜ ë° ë°ì´í„° ë¡œë” ìƒì„±\n",
    "def tensorFromSentence(lang, sentence, tokenizer):\n",
    "    indexes = [SOS_token]\n",
    "    indexes += [lang.word2index.get(word, UNK_token) for word in tokenizer(sentence)[:MAX_LENGTH - 2]]\n",
    "    indexes.append(EOS_token)\n",
    "    # ê¸¸ì´ MAX_LENGTHì— ë§ì¶° PAD ì¶”ê°€\n",
    "    while len(indexes) < MAX_LENGTH:\n",
    "        indexes.append(PAD_token)\n",
    "    return torch.tensor(indexes[:MAX_LENGTH], dtype=torch.long, device=device)\n",
    "\n",
    "def get_dataloader(batch_size):\n",
    "    input_tensors = [tensorFromSentence(input_lang, inp, tokenizer_ko) for inp, _ in pairs]\n",
    "    target_tensors = [tensorFromSentence(output_lang, tgt, tokenizer_en) for _, tgt in pairs]\n",
    "\n",
    "    input_tensors = torch.stack(input_tensors, dim=0)  # [num_samples, MAX_LENGTH]\n",
    "    target_tensors = torch.stack(target_tensors, dim=0)  # [num_samples, MAX_LENGTH]\n",
    "\n",
    "    dataset = TensorDataset(input_tensors, target_tensors)\n",
    "    train_sampler = RandomSampler(dataset)\n",
    "    train_dataloader = DataLoader(dataset, sampler=train_sampler, batch_size=batch_size)\n",
    "\n",
    "    print(f\"input_tensors.shape: {input_tensors.shape}, target_tensors.shape: {target_tensors.shape}\")\n",
    "    return train_dataloader\n",
    "\n",
    "train_dataloader = get_dataloader(batch_size=32)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a4fc910",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "\n",
    "# JSON ë¡œë”©\n",
    "with open('data/train_set.json', encoding='utf-8') as f:\n",
    "    raw_data = json.load(f)['data']\n",
    "\n",
    "# ì›ë¬¸ ì¶”ì¶œ\n",
    "ko_sentences = [item['ko'].strip().replace('>', '') for item in raw_data]\n",
    "en_sentences = [item['mt'].strip().replace('>', '') for item in raw_data]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af76357b",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "ko_lens = [len(s) for s in ko_sentences]\n",
    "en_lens = [len(s) for s in en_sentences]\n",
    "\n",
    "print(f\"ğŸ“ í•œêµ­ì–´ ê¸¸ì´ - í‰ê· : {sum(ko_lens)/len(ko_lens):.2f}, ìµœëŒ€: {max(ko_lens)}, ìµœì†Œ: {min(ko_lens)}\")\n",
    "print(f\"ğŸ“ ì˜ì–´ ê¸¸ì´ - í‰ê· : {sum(en_lens)/len(en_lens):.2f}, ìµœëŒ€: {max(en_lens)}, ìµœì†Œ: {min(en_lens)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2228b20f",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "plt.hist(ko_lens, bins=30, alpha=0.6, label='Korean')\n",
    "plt.hist(en_lens, bins=30, alpha=0.6, label='English')\n",
    "plt.xlabel('ë¬¸ì¥ ê¸¸ì´ (ë¬¸ì ìˆ˜)')\n",
    "plt.ylabel('ë¬¸ì¥ ìˆ˜')\n",
    "plt.title('ë¬¸ì¥ ê¸¸ì´ ë¶„í¬')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa17d9eb",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "plt.scatter(ko_lens, en_lens, alpha=0.4)\n",
    "plt.xlabel('í•œêµ­ì–´ ë¬¸ì¥ ê¸¸ì´')\n",
    "plt.ylabel('ì˜ì–´ ë¬¸ì¥ ê¸¸ì´')\n",
    "plt.title('ë¬¸ì¥ ê¸¸ì´ ìƒê´€ê´€ê³„')\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4776d4cb",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "empty_count = sum(1 for k, e in zip(ko_sentences, en_sentences) if not k or not e)\n",
    "print(f\"â— ë¹ˆ ë¬¸ì¥ ìŒ ìˆ˜: {empty_count}\")\n",
    "\n",
    "pair_set = set(zip(ko_sentences, en_sentences))\n",
    "print(f\"ğŸ§¼ ì „ì²´ ìƒ˜í”Œ: {len(ko_sentences)}, ì¤‘ë³µ ì œê±° í›„: {len(pair_set)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8f75bad",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "domains = [item['domain'] for item in raw_data]\n",
    "domain_count = Counter(domains)\n",
    "\n",
    "plt.bar(domain_count.keys(), domain_count.values())\n",
    "plt.xticks(rotation=45)\n",
    "plt.title(\"ë„ë©”ì¸ ë¶„í¬\")\n",
    "plt.ylabel(\"ìƒ˜í”Œ ìˆ˜\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7372bfb",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "styles = [item['style'] for item in raw_data if item['style']]\n",
    "style_count = Counter(styles)\n",
    "\n",
    "print(\"ğŸ­ ë¬¸ì²´ ë¶„í¬:\", style_count)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aa30e67",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "140b576b",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c26e9d16",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deef7085",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from kiwipiepy import Kiwi\n",
    "import spacy\n",
    "\n",
    "def load_and_tokenize(path):\n",
    "    kiwi = Kiwi()\n",
    "    en_tokenizer = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "    def tokenize_ko(text):\n",
    "        return [token.form for token in kiwi.tokenize(text)]\n",
    "\n",
    "    def tokenize_en(text):\n",
    "        return [token.text.lower() for token in en_tokenizer(text)]\n",
    "\n",
    "    with open(path, encoding='utf-8') as f:\n",
    "        raw = json.load(f)['data']\n",
    "    \n",
    "    ko_raw, en_raw = [], []\n",
    "    ko_tokenized, en_tokenized = [], []\n",
    "\n",
    "    for item in raw:\n",
    "        ko = item['ko'].strip().replace('>', '')\n",
    "        en = item['mt'].strip().replace('>', '')\n",
    "        ko_raw.append(ko)\n",
    "        en_raw.append(en)\n",
    "        ko_tokenized.append(tokenize_ko(ko))\n",
    "        en_tokenized.append(tokenize_en(en))\n",
    "    \n",
    "    return ko_raw, en_raw, ko_tokenized, en_tokenized, raw\n",
    "\n",
    "\n",
    "ko_raw, en_raw, ko_tok, en_tok, raw_json = load_and_tokenize(\"data/train_set.json\")\n",
    "\n",
    "print(\"âœ… í•œêµ­ì–´ ì›ë¬¸:\", ko_raw[0])\n",
    "print(\"âœ… í•œêµ­ì–´ í† í°:\", ko_tok[0])\n",
    "print(\"âœ… ì˜ì–´ ì›ë¬¸:\", en_raw[0])\n",
    "print(\"âœ… ì˜ì–´ í† í°:\", en_tok[0])\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
